{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport warnings\nimport string\n\nfrom tqdm import tqdm\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.linear_model import LinearRegression\n\nfrom lightgbm import LGBMRegressor\n\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-09T05:46:42.113309Z","iopub.execute_input":"2024-01-09T05:46:42.113748Z","iopub.status.idle":"2024-01-09T05:46:42.125361Z","shell.execute_reply.started":"2024-01-09T05:46:42.113713Z","shell.execute_reply":"2024-01-09T05:46:42.123788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'redundant_features': ['up_event'],\n    'feature_rename': {\n        'down_event': 'event_type'\n    }\n}\n\nprint('Reading X Train Data!')\ninput_train_dataset = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n\nprint('Reading X Test Data!')\ninput_test_dataset = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')\n\nprint('Reading Y Train Data!')\ny_train = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:46:42.130600Z","iopub.execute_input":"2024-01-09T05:46:42.131956Z","iopub.status.idle":"2024-01-09T05:47:01.043998Z","shell.execute_reply.started":"2024-01-09T05:46:42.131902Z","shell.execute_reply":"2024-01-09T05:47:01.042551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_essay_paragh(dataframe: pd.DataFrame) -> pd.Series:\n    textInputDf = dataframe[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n\n    def apply_actions(group):\n        essayText = \"\"\n        for activity, cursor_position, text_change in zip(group['activity'], group['cursor_position'], group['text_change']):\n            if activity == 'Replace':\n                replaceTxt = text_change.split(' => ')\n                essayText = essayText[:cursor_position - len(replaceTxt[1])] + replaceTxt[1] + essayText[cursor_position - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            elif activity == 'Paste':\n                essayText = essayText[:cursor_position - len(text_change)] + text_change + essayText[cursor_position - len(text_change):]\n            elif activity == 'Remove/Cut':\n                essayText = essayText[:cursor_position] + essayText[cursor_position + len(text_change):]\n            elif \"M\" in activity:\n                move_info = activity[activity.index('[') + 1:activity.index(']')]\n                move_from, move_to = [int(val) for val in move_info.split(',')]\n                if move_from != move_to:\n                    if move_from < move_to:\n                        essayText = essayText[:move_from] + essayText[move_to:move_to + len(text_change)] + essayText[move_from:move_to] + essayText[move_to + len(text_change):]\n                    else:\n                        essayText = essayText[:move_to] + essayText[move_from:move_from + len(text_change)] + essayText[move_to:move_from] + essayText[move_from + len(text_change):]\n            else:\n                essayText = essayText[:cursor_position - len(text_change)] + text_change + essayText[cursor_position - len(text_change):]\n        return essayText\n\n    essaySeries = textInputDf.groupby('id').apply(apply_actions).to_frame().rename(columns={0: 'essay'}).squeeze()\n\n    return essaySeries\n\ndef q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)\n\ndef split_and_aggregate_sentences(df):\n    AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\n    df['sent'] = df['essay'].str.split('\\\\.|\\\\?|\\\\!')\n    df = df.explode('sent')\n    df['sent'] = df['sent'].str.replace('\\n', '').str.strip()\n    df['sent_len'] = df['sent'].str.len()\n    df['sent_word_count'] = df['sent'].str.split().str.len()\n\n    grouped = df.groupby('id')\n    agg_df = pd.concat([\n        grouped[['sent_len']].agg(AGGREGATIONS),\n        grouped[['sent_word_count']].agg(AGGREGATIONS)\n    ], axis=1)\n\n    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns]\n    agg_df.index.name = 'id'\n    agg_df = agg_df.drop(columns=['sent_word_count_count'])\n    agg_df = agg_df.rename(columns={'sent_len_count': 'sent_count'})\n\n    return agg_df\n\ndef split_and_aggregate_paragraphs(df):\n    AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\n    df['paragraph'] = df['essay'].str.split('\\n')\n    df = df.explode('paragraph')\n\n    df['paragraph_len'] = df['paragraph'].str.len() \n    df['paragraph_word_count'] = df['paragraph'].str.split().str.len()\n\n    grouped = df.groupby('id')\n    agg_df = pd.concat([\n        grouped[['paragraph_len']].agg(AGGREGATIONS),\n        grouped[['paragraph_word_count']].agg(AGGREGATIONS)\n    ], axis=1)\n\n    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns] \n    agg_df.index.name = 'id' \n    agg_df = agg_df.drop(columns=['paragraph_word_count_count']) \n    agg_df = agg_df.rename(columns={'paragraph_len_count': 'paragraph_count'})\n\n    return agg_df\n\ndef get_sentance_level_data(dataframe: pd.DataFrame) -> pd.DataFrame:\n    paragh_data = get_essay_paragh(dataframe.copy())\n    paragh_data = pd.DataFrame({'id': paragh_data.index, 'essay': paragh_data.values})\n    \n    sentance_agg = split_and_aggregate_sentences(paragh_data.copy())\n    paragh_agg = split_and_aggregate_paragraphs(paragh_data.copy())\n\n    master_data = pd.merge(sentance_agg, paragh_agg, on='id')\n\n    return master_data\n\ndef get_activity_counts(dataframe: pd.DataFrame) -> pd.DataFrame:\n    unidentified_columns = [\n        '\\x80', '\\x96', '\\x97', '\\x9b', '¡', '¿', 'Â´', 'Ä±', 'Å\\x9f', 'Ë\\x86', 'â\\x80\\x93', 'ä', 'Unidentified', 'Dead', '0', \n        '1', '2', '5', 'AltGraph', 'Cancel', 'Clear', 'Meta', 'ContextMenu', 'ModeChange', 'OS', 'Pause', 'Process']\n    function_clicks = ['F1', 'F10', 'F11', 'F12', 'F15', 'F2', 'F3', 'F6']\n    mouse_clicks = ['Leftclick', 'Unknownclick', 'Rightclick', 'Middleclick']\n    keyboard_clicks = [\n    'Alt', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'ArrowUp', 'Backspace', 'CapsLock', \n    'Control','Delete', 'End', 'Enter', 'Escape', 'Home', 'Insert', 'NumLock', 'PageDown', 'PageUp', \n    'ScrollLock', 'Shift', 'Space', 'Tab']\n    redundent_activity = [\n    'AudioVolumeDown', 'AudioVolumeMute', 'AudioVolumeUp','MediaPlayPause', 'MediaTrackNext', 'MediaTrackPrevious']\n    \n    dataframe = dataframe.groupby(['id', 'down_event']).size().reset_index(name='count')\n    dataframe = dataframe.pivot_table(index='id', columns='down_event', values='count', fill_value=0).reset_index()\n    \n    punct_columns = dataframe.columns[dataframe.columns.isin(list(string.punctuation))]\n    input_columns = dataframe.columns[dataframe.columns.isin(list(string.ascii_lowercase) + list(string.ascii_uppercase))]\n    unidnty_columns = dataframe.columns[dataframe.columns.isin(unidentified_columns)]\n    func_columns = dataframe.columns[dataframe.columns.isin(function_clicks)]\n    mouse_columns = dataframe.columns[dataframe.columns.isin(mouse_clicks)]\n    keyboard_columns = dataframe.columns[dataframe.columns.isin(keyboard_clicks)]\n    redundant_columns = dataframe.columns[dataframe.columns.isin(redundent_activity)]\n    \n    dataframe['punctuation'] = dataframe[punct_columns].sum(axis=1)\n    dataframe['inputs'] = dataframe[input_columns].sum(axis=1)\n    dataframe['unidentified'] = dataframe[unidnty_columns].sum(axis=1)\n    dataframe['functions'] = dataframe[func_columns].sum(axis=1)\n    dataframe['mouse_clicks'] = dataframe[mouse_columns].sum(axis=1)\n    dataframe['keyboard_clicks'] = dataframe[keyboard_columns].sum(axis=1)\n    dataframe['redundant'] = dataframe[redundant_columns].sum(axis=1)\n\n    columns_to_drop = list(punct_columns) + list(input_columns) + list(unidnty_columns) + list(func_columns) + list(mouse_columns) + list(keyboard_columns) + list(redundant_columns) \n    \n    dataframe = dataframe.drop(columns=columns_to_drop)\n    dataframe = dataframe[[\n        'id', 'punctuation', 'inputs', 'unidentified', 'functions', 'mouse_clicks', 'keyboard_clicks', 'redundant']]\n\n    return dataframe.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.047419Z","iopub.execute_input":"2024-01-09T05:47:01.047950Z","iopub.status.idle":"2024-01-09T05:47:01.092125Z","shell.execute_reply.started":"2024-01-09T05:47:01.047904Z","shell.execute_reply":"2024-01-09T05:47:01.090674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_clean_data(X: pd.DataFrame, feature_list: list, rename_dict: dict) -> pd.DataFrame:\n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Nonproduction'), 'down_event'] = 'NoEvent'\n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Nonproduction'), 'up_event'] = 'NoEvent'\n    \n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Input'), 'up_event'] = 'q'\n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Replace'), 'up_event'] = 'q'\n\n    X.loc[X['activity'].str.contains('Move From'), 'activity'] = 'MoveSection'\n\n    X = X.drop(columns=feature_list)\n    X = X.rename(columns=rename_dict)\n\n    return X\n\ndef rounded_rmse(y, y_pred, **kwargs):\n    return mean_squared_error(y, np.round(y_pred * 2) / 2, squared=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.094536Z","iopub.execute_input":"2024-01-09T05:47:01.094995Z","iopub.status.idle":"2024-01-09T05:47:01.111180Z","shell.execute_reply.started":"2024-01-09T05:47:01.094959Z","shell.execute_reply":"2024-01-09T05:47:01.110009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineering:\n\n    @staticmethod\n    def get_capitalized_letters(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_event_type'] = X['event_type'].shift()\n        X['capitalize_letters'] = (X['activity'] == 'Input') & (X['previous_event_type'] == 'Shift') & (X['event_type'] == 'q')\n        \n        X = X.drop(columns=['previous_event_type'])\n        \n        return X\n\n    @staticmethod\n    def get_temporal_features(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_up_time'] = X['up_time'].shift().fillna(X['down_time'].iloc[0])\n        X['time_between_events'] = X['down_time'] - X['previous_up_time']\n        \n        X['cumulative_writing_time'] = (X['action_time'] + X['time_between_events']).cumsum()\n\n        X['warning_issued'] = X['time_between_events'] >= 120000\n        X = X.drop(columns=['previous_up_time'])\n        \n        return X\n\n    @staticmethod\n    def get_cursor_features(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_cursor_position'] = X['cursor_position'].shift().fillna(0)\n        X['cursor_move_distance'] = X['cursor_position'] - X['previous_cursor_position']\n        X['cursor_move_distance'] = X['cursor_move_distance'].abs()\n\n        X = X.drop(columns=['previous_cursor_position'])\n\n        return X\n\n    @staticmethod\n    def get_word_change_features(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_word_count'] = X['word_count'].shift().fillna(0)\n        X['word_count_change'] = X['word_count'] - X['previous_word_count']\n        X['word_count_change'] = X['word_count_change'].abs()\n\n        X = X.drop(columns=['previous_word_count'])\n\n        return X","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.115663Z","iopub.execute_input":"2024-01-09T05:47:01.116708Z","iopub.status.idle":"2024-01-09T05:47:01.131173Z","shell.execute_reply.started":"2024-01-09T05:47:01.116632Z","shell.execute_reply":"2024-01-09T05:47:01.129692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_features(unique_dataset):\n    feature_list = [\n        'id', 'total_number_of_events', 'final_number_of_words', 'number_of_warnings_issued',\n        'total_time_taken', 'total_pause_time', 'average_pause_length', 'proportion_pause_time',\n        'non_productive_events', 'input_events', 'deletion_events', 'addition_events', 'replacement_events', 'string_move_events',\n        'number_of_sentences', 'average_action_time', 'median_action_time', 'min_action_time', 'max_action_time',\n        'std_action_time', 'sum_action_time', 'average_cursor_distance', 'max_cursor_distance', 'total_cursor_distance', 'std_cursor_distance', \n        'avg_word_count_btw_events', 'min_time_between_events', 'max_time_between_events', 'std_time_between_events'\n    ]\n    \n    data_values = []\n\n    data_values.append(unique_dataset['id'].iloc[0])\n    data_values.append(unique_dataset['event_id'].iloc[-1])\n    data_values.append(unique_dataset['word_count'].iloc[-1])\n    data_values.append(unique_dataset['warning_issued'].sum())\n    data_values.append(unique_dataset['cumulative_writing_time'].iloc[-1])\n    data_values.append(unique_dataset['time_between_events'].sum())\n\n    data_values.append(unique_dataset['time_between_events'].mean())\n    data_values.append(unique_dataset['time_between_events'].sum() / unique_dataset['cumulative_writing_time'].iloc[-1])\n\n    data_values.extend([\n        unique_dataset[unique_dataset['activity'] == 'Nonproduction'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Input'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Remove/Cut'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Paste'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Replace'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'MoveSection'].shape[0],\n    ])\n\n    data_values.append(unique_dataset[unique_dataset['event_type'] == '.'].shape[0])\n    data_values.append(unique_dataset['action_time'].mean())\n    data_values.append(unique_dataset['action_time'].median())\n    data_values.append(unique_dataset['action_time'].min())\n    data_values.append(unique_dataset['action_time'].max())\n    data_values.append(unique_dataset['action_time'].std())\n    data_values.append(unique_dataset['action_time'].sum())\n\n    data_values.append(unique_dataset['cursor_move_distance'].mean())\n    data_values.append(unique_dataset['cursor_move_distance'].max())\n    data_values.append(unique_dataset['cursor_move_distance'].sum())\n    data_values.append(unique_dataset['cursor_move_distance'].std())\n    \n    data_values.append(unique_dataset['word_count_change'].mean())\n    \n    data_values.append(unique_dataset['time_between_events'].min())\n    data_values.append(unique_dataset['time_between_events'].max())\n    data_values.append(unique_dataset['time_between_events'].std())\n\n    return pd.Series(data_values, index=feature_list)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.133029Z","iopub.execute_input":"2024-01-09T05:47:01.133548Z","iopub.status.idle":"2024-01-09T05:47:01.153644Z","shell.execute_reply.started":"2024-01-09T05:47:01.133509Z","shell.execute_reply":"2024-01-09T05:47:01.152140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_master_data(input_data: pd.DataFrame, config: dict) -> pd.DataFrame:\n    print('Cleaning Train Dataset!')\n    \n    sent_paragh_data = get_sentance_level_data(input_data.copy())\n    activity_count_data = get_activity_counts(input_data.copy())\n    \n    cleaned_data = get_clean_data(input_data, config['redundant_features'], config['feature_rename'])\n\n    print('Preprocessing Train Data!')\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_capitalized_letters)\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_temporal_features)\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_cursor_features)\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_word_change_features)\n\n    master_data = cleaned_data.groupby('id').apply(calculate_features).reset_index(drop=True)\n\n    master_data = pd.merge(master_data, sent_paragh_data, on='id')\n    master_data = pd.merge(master_data, activity_count_data, on='id')\n\n    master_data['total_writing_time'] = master_data['total_time_taken'] - master_data['total_pause_time']\n\n    master_data['proportion_np_events'] = master_data['non_productive_events'] / master_data['total_number_of_events']\n    master_data['proportion_input_events'] = master_data['input_events'] / master_data['total_number_of_events']\n    master_data['proportion_delete_events'] = master_data['deletion_events'] / master_data['total_number_of_events']\n    master_data['proportion_addition_events'] = master_data['addition_events'] / master_data['total_number_of_events']\n    master_data['proportion_replace_events'] = master_data['replacement_events'] / master_data['total_number_of_events']\n    master_data['proportion_moving_events'] = master_data['string_move_events'] / master_data['total_number_of_events']\n\n    print('Preprocessing Complete!')\n    \n    return master_data","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.155652Z","iopub.execute_input":"2024-01-09T05:47:01.156116Z","iopub.status.idle":"2024-01-09T05:47:01.172008Z","shell.execute_reply.started":"2024-01-09T05:47:01.156075Z","shell.execute_reply":"2024-01-09T05:47:01.170938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_corr_features(dataframe: pd.DataFrame) -> list:\n    corr_matrix = dataframe.corr().abs()\n    \n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n    \n    return to_drop","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.174350Z","iopub.execute_input":"2024-01-09T05:47:01.174891Z","iopub.status.idle":"2024-01-09T05:47:01.191935Z","shell.execute_reply.started":"2024-01-09T05:47:01.174821Z","shell.execute_reply":"2024-01-09T05:47:01.190946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train Data!')\n\nmaster_data = create_master_data(input_data=input_train_dataset, config=config)\nmaster_data = pd.merge(master_data, y_train, on='id')\n\nprint('Test Data!')\n\nmaster_data_test = create_master_data(input_data=input_test_dataset, config=config)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:47:01.193592Z","iopub.execute_input":"2024-01-09T05:47:01.193994Z","iopub.status.idle":"2024-01-09T05:49:22.116044Z","shell.execute_reply.started":"2024-01-09T05:47:01.193952Z","shell.execute_reply":"2024-01-09T05:49:22.114889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Creating X and y Dataframes!')\n\nmaster_data = master_data.set_index('id')\nmaster_data_test = master_data_test.set_index('id')\n\ny = master_data['score']\nX = master_data.drop(columns=['score'])\n\ndrop_columns = remove_corr_features(X)\nX = X.drop(columns=drop_columns)\nmaster_data_test = master_data_test.drop(columns=drop_columns)\n\nprint(f'No. of Features: {X.shape[1]}')\nprint(f'Feature List: {X.columns}')\n\nscalar = StandardScaler()\ntransformer = PowerTransformer()\n\nX_train = transformer.fit_transform(scalar.fit_transform(X))\nX_test = transformer.transform(scalar.transform(master_data_test))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:49:22.117567Z","iopub.execute_input":"2024-01-09T05:49:22.117933Z","iopub.status.idle":"2024-01-09T05:49:22.559902Z","shell.execute_reply.started":"2024-01-09T05:49:22.117902Z","shell.execute_reply":"2024-01-09T05:49:22.558282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RegressorEnsemble(BaseEstimator, RegressorMixin):\n\n    def __init__(self, model_params: dict, models_list: list = None):\n        self.models_list = [\n            ('gbr', GradientBoostingRegressor(random_state=0, **model_params['gbr'])),\n            ('rfr', RandomForestRegressor(random_state=0, **model_params['rfr'])),\n            ('lgbm', LGBMRegressor(random_state=0, **model_params['lgbm'])),\n        ] if models_list is None else models_list\n        \n        self.blending_model = None\n\n    def fit(self, X, y=None):\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=0)\n        meta_X = list()\n        \n        for _, model_object in self.models_list:\n            model_object.fit(X_train, y_train)\n            yhat = model_object.predict(X_val)\n            \n            yhat = yhat.reshape(len(yhat), 1)\n            meta_X.append(yhat)\n            \n        self.blending_model = LinearRegression().fit(np.hstack(meta_X), y_val)\n        \n        return self\n    \n    def predict(self, X, y=None):\n        meta_X = list()\n        \n        for _, model_object in self.models_list:\n            yhat = model_object.predict(X)\n            \n            yhat = yhat.reshape(len(yhat), 1)\n            meta_X.append(yhat)\n            \n        return self.blending_model.predict(np.hstack(meta_X))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:49:22.564138Z","iopub.execute_input":"2024-01-09T05:49:22.564605Z","iopub.status.idle":"2024-01-09T05:49:22.579217Z","shell.execute_reply.started":"2024-01-09T05:49:22.564563Z","shell.execute_reply":"2024-01-09T05:49:22.577613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Building Model Object!')\n\nmodel_params = {'gbr': {'n_estimators': 288, 'learning_rate': 0.06240526278745896, 'max_depth': 6, 'min_samples_split': 0.3900536752974622, 'min_samples_leaf': 0.3647733862960716, 'subsample': 0.9038293435634874}, 'rfr': {'n_estimators': 133, 'max_depth': 20, 'min_samples_split': 0.3728805516681204, 'min_samples_leaf': 0.3983901408168059}, 'lgbm': {'boosting_type': 'gbdt', 'n_estimators': 119, 'learning_rate': 0.09299717099237738, 'num_leaves': 73, 'max_depth': 4, 'min_child_samples': 16, 'subsample': 0.7806461807427068, 'colsample_bytree': 0.674188872051343, 'reg_alpha': 0.4383399140886416, 'reg_lambda': 0.5283701867343921}}\n\nreg_model = RegressorEnsemble(model_params=model_params).fit(X_train, y)\n\ny_hat_train = np.round(reg_model.predict(X_train), 3)\ny_hat_test = np.round(reg_model.predict(X_test), 3)\n\nsubmission_data = pd.DataFrame({'id': master_data_test.index, 'score': y_hat_test})\n\nprint('Previous Best:: R^2 Score: 0.861, RMSE Score: 0.382, Rounded RMSE Score: 0.39')\n\nprint(f'R^2 Score: {round(reg_model.score(X_train, y), 3)},', \n      f'RMSE Score: {round(mean_squared_error(y, y_hat_train, squared=False), 3)},',\n      f'Rounded RMSE Score: {round(rounded_rmse(y, y_hat_train), 3)}')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:49:22.581087Z","iopub.execute_input":"2024-01-09T05:49:22.581568Z","iopub.status.idle":"2024-01-09T05:49:25.082804Z","shell.execute_reply.started":"2024-01-09T05:49:22.581535Z","shell.execute_reply":"2024-01-09T05:49:25.081022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:49:25.084919Z","iopub.execute_input":"2024-01-09T05:49:25.085561Z","iopub.status.idle":"2024-01-09T05:49:25.109267Z","shell.execute_reply.started":"2024-01-09T05:49:25.085505Z","shell.execute_reply":"2024-01-09T05:49:25.107701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:49:25.111279Z","iopub.execute_input":"2024-01-09T05:49:25.112666Z","iopub.status.idle":"2024-01-09T05:49:25.123397Z","shell.execute_reply.started":"2024-01-09T05:49:25.112602Z","shell.execute_reply":"2024-01-09T05:49:25.121574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}