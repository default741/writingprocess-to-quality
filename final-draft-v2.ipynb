{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport warnings\nimport string\n\nfrom tqdm import tqdm\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.linear_model import LinearRegression\n\nfrom lightgbm import LGBMRegressor\n\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-09T23:40:04.081501Z","iopub.execute_input":"2024-01-09T23:40:04.082293Z","iopub.status.idle":"2024-01-09T23:40:04.094526Z","shell.execute_reply.started":"2024-01-09T23:40:04.082252Z","shell.execute_reply":"2024-01-09T23:40:04.093237Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"config = {\n    'redundant_features': ['up_event'],\n    'feature_rename': {\n        'down_event': 'event_type'\n    }\n}\n\nprint('Reading X Train Data!')\ninput_train_dataset = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n\nprint('Reading X Test Data!')\ninput_test_dataset = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')\n\nprint('Reading Y Train Data!')\ny_train = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:04.096413Z","iopub.execute_input":"2024-01-09T23:40:04.096747Z","iopub.status.idle":"2024-01-09T23:40:16.522173Z","shell.execute_reply.started":"2024-01-09T23:40:04.096719Z","shell.execute_reply":"2024-01-09T23:40:16.521070Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Reading X Train Data!\nReading X Test Data!\nReading Y Train Data!\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_essay_paragh(dataframe: pd.DataFrame) -> pd.Series:\n    textInputDf = dataframe[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n\n    def apply_actions(group):\n        essayText = \"\"\n        for activity, cursor_position, text_change in zip(group['activity'], group['cursor_position'], group['text_change']):\n            if activity == 'Replace':\n                replaceTxt = text_change.split(' => ')\n                essayText = essayText[:cursor_position - len(replaceTxt[1])] + replaceTxt[1] + essayText[cursor_position - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            elif activity == 'Paste':\n                essayText = essayText[:cursor_position - len(text_change)] + text_change + essayText[cursor_position - len(text_change):]\n            elif activity == 'Remove/Cut':\n                essayText = essayText[:cursor_position] + essayText[cursor_position + len(text_change):]\n            elif \"M\" in activity:\n                move_info = activity[activity.index('[') + 1:activity.index(']')]\n                move_from, move_to = [int(val) for val in move_info.split(',')]\n                if move_from != move_to:\n                    if move_from < move_to:\n                        essayText = essayText[:move_from] + essayText[move_to:move_to + len(text_change)] + essayText[move_from:move_to] + essayText[move_to + len(text_change):]\n                    else:\n                        essayText = essayText[:move_to] + essayText[move_from:move_from + len(text_change)] + essayText[move_to:move_from] + essayText[move_from + len(text_change):]\n            else:\n                essayText = essayText[:cursor_position - len(text_change)] + text_change + essayText[cursor_position - len(text_change):]\n        return essayText\n\n    essaySeries = textInputDf.groupby('id').apply(apply_actions).to_frame().rename(columns={0: 'essay'}).squeeze()\n\n    return essaySeries\n\ndef q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)\n\ndef split_and_aggregate_sentences(df):\n    AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\n    df['sent'] = df['essay'].str.split('\\\\.|\\\\?|\\\\!')\n    df = df.explode('sent')\n    df['sent'] = df['sent'].str.replace('\\n', '').str.strip()\n    df['sent_len'] = df['sent'].str.len()\n    df['sent_word_count'] = df['sent'].str.split().str.len()\n\n    grouped = df.groupby('id')\n    agg_df = pd.concat([\n        grouped[['sent_len']].agg(AGGREGATIONS),\n        grouped[['sent_word_count']].agg(AGGREGATIONS)\n    ], axis=1)\n\n    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns]\n    agg_df.index.name = 'id'\n    agg_df = agg_df.drop(columns=['sent_word_count_count'])\n    agg_df = agg_df.rename(columns={'sent_len_count': 'sent_count'})\n\n    return agg_df\n\ndef split_and_aggregate_paragraphs(df):\n    AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\n    df['paragraph'] = df['essay'].str.split('\\n')\n    df = df.explode('paragraph')\n\n    df['paragraph_len'] = df['paragraph'].str.len() \n    df['paragraph_word_count'] = df['paragraph'].str.split().str.len()\n\n    grouped = df.groupby('id')\n    agg_df = pd.concat([\n        grouped[['paragraph_len']].agg(AGGREGATIONS),\n        grouped[['paragraph_word_count']].agg(AGGREGATIONS)\n    ], axis=1)\n\n    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns] \n    agg_df.index.name = 'id' \n    agg_df = agg_df.drop(columns=['paragraph_word_count_count']) \n    agg_df = agg_df.rename(columns={'paragraph_len_count': 'paragraph_count'})\n\n    return agg_df\n\ndef get_sentance_level_data(dataframe: pd.DataFrame) -> pd.DataFrame:\n    paragh_data = get_essay_paragh(dataframe.copy())\n    paragh_data = pd.DataFrame({'id': paragh_data.index, 'essay': paragh_data.values})\n    \n    sentance_agg = split_and_aggregate_sentences(paragh_data.copy())\n    paragh_agg = split_and_aggregate_paragraphs(paragh_data.copy())\n\n    master_data = pd.merge(sentance_agg, paragh_agg, on='id')\n\n    return master_data\n\ndef get_activity_counts(dataframe: pd.DataFrame) -> pd.DataFrame:\n    unidentified_columns = [\n        '\\x80', '\\x96', '\\x97', '\\x9b', '¡', '¿', 'Â´', 'Ä±', 'Å\\x9f', 'Ë\\x86', 'â\\x80\\x93', 'ä', 'Unidentified', 'Dead', '0', \n        '1', '2', '5', 'AltGraph', 'Cancel', 'Clear', 'Meta', 'ContextMenu', 'ModeChange', 'OS', 'Pause', 'Process']\n    function_clicks = ['F1', 'F10', 'F11', 'F12', 'F15', 'F2', 'F3', 'F6']\n    mouse_clicks = ['Leftclick', 'Unknownclick', 'Rightclick', 'Middleclick']\n    keyboard_clicks = [\n    'Alt', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'ArrowUp', 'Backspace', 'CapsLock', \n    'Control','Delete', 'End', 'Enter', 'Escape', 'Home', 'Insert', 'NumLock', 'PageDown', 'PageUp', \n    'ScrollLock', 'Shift', 'Space', 'Tab']\n    redundent_activity = [\n    'AudioVolumeDown', 'AudioVolumeMute', 'AudioVolumeUp','MediaPlayPause', 'MediaTrackNext', 'MediaTrackPrevious']\n    \n    dataframe = dataframe.groupby(['id', 'down_event']).size().reset_index(name='count')\n    dataframe = dataframe.pivot_table(index='id', columns='down_event', values='count', fill_value=0).reset_index()\n    \n    punct_columns = dataframe.columns[dataframe.columns.isin(list(string.punctuation))]\n    input_columns = dataframe.columns[dataframe.columns.isin(list(string.ascii_lowercase) + list(string.ascii_uppercase))]\n    unidnty_columns = dataframe.columns[dataframe.columns.isin(unidentified_columns)]\n    func_columns = dataframe.columns[dataframe.columns.isin(function_clicks)]\n    mouse_columns = dataframe.columns[dataframe.columns.isin(mouse_clicks)]\n    keyboard_columns = dataframe.columns[dataframe.columns.isin(keyboard_clicks)]\n    redundant_columns = dataframe.columns[dataframe.columns.isin(redundent_activity)]\n    \n    dataframe['punctuation'] = dataframe[punct_columns].sum(axis=1)\n    dataframe['inputs'] = dataframe[input_columns].sum(axis=1)\n    dataframe['unidentified'] = dataframe[unidnty_columns].sum(axis=1)\n    dataframe['functions'] = dataframe[func_columns].sum(axis=1)\n    dataframe['mouse_clicks'] = dataframe[mouse_columns].sum(axis=1)\n    dataframe['keyboard_clicks'] = dataframe[keyboard_columns].sum(axis=1)\n    dataframe['redundant'] = dataframe[redundant_columns].sum(axis=1)\n\n    columns_to_drop = list(punct_columns) + list(input_columns) + list(unidnty_columns) + list(func_columns) + list(mouse_columns) + list(keyboard_columns) + list(redundant_columns) \n    \n    dataframe = dataframe.drop(columns=columns_to_drop)\n    dataframe = dataframe[[\n        'id', 'punctuation', 'inputs', 'unidentified', 'functions', 'mouse_clicks', 'keyboard_clicks', 'redundant']]\n\n    return dataframe.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.523941Z","iopub.execute_input":"2024-01-09T23:40:16.524625Z","iopub.status.idle":"2024-01-09T23:40:16.563597Z","shell.execute_reply.started":"2024-01-09T23:40:16.524592Z","shell.execute_reply":"2024-01-09T23:40:16.562439Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_clean_data(X: pd.DataFrame, feature_list: list, rename_dict: dict) -> pd.DataFrame:\n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Nonproduction'), 'down_event'] = 'NoEvent'\n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Nonproduction'), 'up_event'] = 'NoEvent'\n    \n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Input'), 'up_event'] = 'q'\n    X.loc[(X['up_event'] != X['down_event']) & (X['activity'] == 'Replace'), 'up_event'] = 'q'\n\n    X.loc[X['activity'].str.contains('Move From'), 'activity'] = 'MoveSection'\n\n    X = X.drop(columns=feature_list)\n    X = X.rename(columns=rename_dict)\n\n    return X\n\ndef rounded_rmse(y, y_pred, **kwargs):\n    return mean_squared_error(y, np.round(y_pred * 2) / 2, squared=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.565268Z","iopub.execute_input":"2024-01-09T23:40:16.565593Z","iopub.status.idle":"2024-01-09T23:40:16.581130Z","shell.execute_reply.started":"2024-01-09T23:40:16.565560Z","shell.execute_reply":"2024-01-09T23:40:16.580004Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineering:\n\n    @staticmethod\n    def get_capitalized_letters(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_event_type'] = X['event_type'].shift()\n        X['capitalize_letters'] = (X['activity'] == 'Input') & (X['previous_event_type'] == 'Shift') & (X['event_type'] == 'q')\n        \n        X = X.drop(columns=['previous_event_type'])\n        \n        return X\n\n    @staticmethod\n    def get_temporal_features(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_up_time'] = X['up_time'].shift().fillna(X['down_time'].iloc[0])\n        X['time_between_events'] = X['down_time'] - X['previous_up_time']\n        \n        X['cumulative_writing_time'] = (X['action_time'] + X['time_between_events']).cumsum()\n\n        X['warning_issued'] = X['time_between_events'] >= 120000\n        X = X.drop(columns=['previous_up_time'])\n        \n        return X\n\n    @staticmethod\n    def get_cursor_features(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_cursor_position'] = X['cursor_position'].shift().fillna(0)\n        X['cursor_move_distance'] = X['cursor_position'] - X['previous_cursor_position']\n        X['cursor_move_distance'] = X['cursor_move_distance'].abs()\n\n        X = X.drop(columns=['previous_cursor_position'])\n\n        return X\n\n    @staticmethod\n    def get_word_change_features(X: pd.DataFrame) -> pd.DataFrame:\n        X['previous_word_count'] = X['word_count'].shift().fillna(0)\n        X['word_count_change'] = X['word_count'] - X['previous_word_count']\n        X['word_count_change'] = X['word_count_change'].abs()\n\n        X = X.drop(columns=['previous_word_count'])\n\n        return X","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.583876Z","iopub.execute_input":"2024-01-09T23:40:16.584230Z","iopub.status.idle":"2024-01-09T23:40:16.596926Z","shell.execute_reply.started":"2024-01-09T23:40:16.584200Z","shell.execute_reply":"2024-01-09T23:40:16.596085Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def calculate_features(unique_dataset):\n    feature_list = [\n        'id', 'total_number_of_events', 'final_number_of_words', 'number_of_warnings_issued',\n        'total_time_taken', 'total_pause_time', 'average_pause_length', 'proportion_pause_time',\n        'non_productive_events', 'input_events', 'deletion_events', 'addition_events', 'replacement_events', 'string_move_events',\n        'number_of_sentences', 'average_action_time', 'median_action_time', 'min_action_time', 'max_action_time',\n        'std_action_time', 'sum_action_time', 'average_cursor_distance', 'max_cursor_distance', 'total_cursor_distance', 'std_cursor_distance', \n        'avg_word_count_btw_events', 'min_time_between_events', 'max_time_between_events', 'std_time_between_events'\n    ]\n    \n    data_values = []\n\n    data_values.append(unique_dataset['id'].iloc[0])\n    data_values.append(unique_dataset['event_id'].iloc[-1])\n    data_values.append(unique_dataset['word_count'].iloc[-1])\n    data_values.append(unique_dataset['warning_issued'].sum())\n    data_values.append(unique_dataset['cumulative_writing_time'].iloc[-1])\n    data_values.append(unique_dataset['time_between_events'].sum())\n\n    data_values.append(unique_dataset['time_between_events'].mean())\n    data_values.append(unique_dataset['time_between_events'].sum() / unique_dataset['cumulative_writing_time'].iloc[-1])\n\n    data_values.extend([\n        unique_dataset[unique_dataset['activity'] == 'Nonproduction'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Input'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Remove/Cut'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Paste'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'Replace'].shape[0],\n        unique_dataset[unique_dataset['activity'] == 'MoveSection'].shape[0],\n    ])\n\n    data_values.append(unique_dataset[unique_dataset['event_type'] == '.'].shape[0])\n    data_values.append(unique_dataset['action_time'].mean())\n    data_values.append(unique_dataset['action_time'].median())\n    data_values.append(unique_dataset['action_time'].min())\n    data_values.append(unique_dataset['action_time'].max())\n    data_values.append(unique_dataset['action_time'].std())\n    data_values.append(unique_dataset['action_time'].sum())\n\n    data_values.append(unique_dataset['cursor_move_distance'].mean())\n    data_values.append(unique_dataset['cursor_move_distance'].max())\n    data_values.append(unique_dataset['cursor_move_distance'].sum())\n    data_values.append(unique_dataset['cursor_move_distance'].std())\n    \n    data_values.append(unique_dataset['word_count_change'].mean())\n    \n    data_values.append(unique_dataset['time_between_events'].min())\n    data_values.append(unique_dataset['time_between_events'].max())\n    data_values.append(unique_dataset['time_between_events'].std())\n\n    return pd.Series(data_values, index=feature_list)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.597999Z","iopub.execute_input":"2024-01-09T23:40:16.599083Z","iopub.status.idle":"2024-01-09T23:40:16.615832Z","shell.execute_reply.started":"2024-01-09T23:40:16.599052Z","shell.execute_reply":"2024-01-09T23:40:16.614885Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def create_master_data(input_data: pd.DataFrame, config: dict) -> pd.DataFrame:\n    print('Cleaning Train Dataset!')\n    \n    sent_paragh_data = get_sentance_level_data(input_data.copy())\n    activity_count_data = get_activity_counts(input_data.copy())\n    \n    cleaned_data = get_clean_data(input_data, config['redundant_features'], config['feature_rename'])\n\n    print('Preprocessing Train Data!')\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_capitalized_letters)\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_temporal_features)\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_cursor_features)\n    cleaned_data = cleaned_data.groupby('id', group_keys=False, sort=False).apply(FeatureEngineering.get_word_change_features)\n\n    master_data = cleaned_data.groupby('id').apply(calculate_features).reset_index(drop=True)\n\n    master_data = pd.merge(master_data, sent_paragh_data, on='id')\n    master_data = pd.merge(master_data, activity_count_data, on='id')\n\n    master_data['total_writing_time'] = master_data['total_time_taken'] - master_data['total_pause_time']\n\n    master_data['proportion_np_events'] = master_data['non_productive_events'] / master_data['total_number_of_events']\n    master_data['proportion_input_events'] = master_data['input_events'] / master_data['total_number_of_events']\n    master_data['proportion_delete_events'] = master_data['deletion_events'] / master_data['total_number_of_events']\n    master_data['proportion_addition_events'] = master_data['addition_events'] / master_data['total_number_of_events']\n    master_data['proportion_replace_events'] = master_data['replacement_events'] / master_data['total_number_of_events']\n    master_data['proportion_moving_events'] = master_data['string_move_events'] / master_data['total_number_of_events']\n\n    print('Preprocessing Complete!')\n    \n    return master_data","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.617520Z","iopub.execute_input":"2024-01-09T23:40:16.617845Z","iopub.status.idle":"2024-01-09T23:40:16.631315Z","shell.execute_reply.started":"2024-01-09T23:40:16.617818Z","shell.execute_reply":"2024-01-09T23:40:16.630207Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def remove_corr_features(dataframe: pd.DataFrame) -> list:\n    corr_matrix = dataframe.corr().abs()\n    \n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n    \n    return to_drop","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.632582Z","iopub.execute_input":"2024-01-09T23:40:16.632993Z","iopub.status.idle":"2024-01-09T23:40:16.645130Z","shell.execute_reply.started":"2024-01-09T23:40:16.632964Z","shell.execute_reply":"2024-01-09T23:40:16.644292Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print('Train Data!')\n\nmaster_data = create_master_data(input_data=input_train_dataset, config=config)\nmaster_data = pd.merge(master_data, y_train, on='id')\n\nprint('Test Data!')\n\nmaster_data_test = create_master_data(input_data=input_test_dataset, config=config)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:40:16.646387Z","iopub.execute_input":"2024-01-09T23:40:16.646907Z","iopub.status.idle":"2024-01-09T23:42:30.421214Z","shell.execute_reply.started":"2024-01-09T23:40:16.646877Z","shell.execute_reply":"2024-01-09T23:42:30.420196Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Train Data!\nCleaning Train Dataset!\nPreprocessing Train Data!\nPreprocessing Complete!\nTest Data!\nCleaning Train Dataset!\nPreprocessing Train Data!\nPreprocessing Complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Creating X and y Dataframes!')\n\nmaster_data = master_data.set_index('id')\nmaster_data_test = master_data_test.set_index('id')\n\ny = master_data['score']\nX = master_data.drop(columns=['score'])\n\ndrop_columns = remove_corr_features(X)\nX = X.drop(columns=drop_columns)\nmaster_data_test = master_data_test.drop(columns=drop_columns)\n\nprint(f'No. of Features: {X.shape[1]}')\nprint(f'Feature List: {X.columns}')\n\nscalar = StandardScaler()\ntransformer = PowerTransformer()\n\nX_train = transformer.fit_transform(scalar.fit_transform(X))\nX_test = transformer.transform(scalar.transform(master_data_test))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:42:30.422500Z","iopub.execute_input":"2024-01-09T23:42:30.422797Z","iopub.status.idle":"2024-01-09T23:42:30.881481Z","shell.execute_reply.started":"2024-01-09T23:42:30.422771Z","shell.execute_reply":"2024-01-09T23:42:30.880335Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Creating X and y Dataframes!\nNo. of Features: 56\nFeature List: Index(['total_number_of_events', 'final_number_of_words',\n       'number_of_warnings_issued', 'total_time_taken', 'total_pause_time',\n       'average_pause_length', 'proportion_pause_time',\n       'non_productive_events', 'input_events', 'deletion_events',\n       'addition_events', 'replacement_events', 'string_move_events',\n       'number_of_sentences', 'average_action_time', 'median_action_time',\n       'min_action_time', 'max_action_time', 'sum_action_time',\n       'average_cursor_distance', 'max_cursor_distance',\n       'total_cursor_distance', 'std_cursor_distance',\n       'avg_word_count_btw_events', 'min_time_between_events',\n       'max_time_between_events', 'std_time_between_events', 'sent_count',\n       'sent_len_mean', 'sent_len_min', 'sent_len_max', 'sent_len_first',\n       'sent_len_last', 'sent_len_q1', 'sent_len_median', 'paragraph_count',\n       'paragraph_len_mean', 'paragraph_len_min', 'paragraph_len_max',\n       'paragraph_len_first', 'paragraph_len_last', 'paragraph_len_q1',\n       'paragraph_len_median', 'paragraph_len_q3', 'punctuation',\n       'unidentified', 'functions', 'mouse_clicks', 'keyboard_clicks',\n       'redundant', 'proportion_np_events', 'proportion_input_events',\n       'proportion_delete_events', 'proportion_addition_events',\n       'proportion_replace_events', 'proportion_moving_events'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"class RegressorEnsemble(BaseEstimator, RegressorMixin):\n\n    def __init__(self, model_params: dict, models_list: list = None):\n        self.models_list = [\n            ('gbr', GradientBoostingRegressor(random_state=0, **model_params['gbr'])),\n            ('rfr', RandomForestRegressor(random_state=0, **model_params['rfr'])),\n            ('lgbm', LGBMRegressor(random_state=0, **model_params['lgbm'])),\n        ] if models_list is None else models_list\n        \n        self.blending_model = None\n\n    def fit(self, X, y=None):\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=0)\n        meta_X = list()\n        \n        for _, model_object in self.models_list:\n            model_object.fit(X_train, y_train)\n            yhat = model_object.predict(X_val)\n            \n            yhat = yhat.reshape(len(yhat), 1)\n            meta_X.append(yhat)\n            \n        self.blending_model = LinearRegression().fit(np.hstack(meta_X), y_val)\n        \n        return self\n    \n    def predict(self, X, y=None):\n        meta_X = list()\n        \n        for _, model_object in self.models_list:\n            yhat = model_object.predict(X)\n            \n            yhat = yhat.reshape(len(yhat), 1)\n            meta_X.append(yhat)\n            \n        return self.blending_model.predict(np.hstack(meta_X))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:42:30.882764Z","iopub.execute_input":"2024-01-09T23:42:30.883107Z","iopub.status.idle":"2024-01-09T23:42:30.894455Z","shell.execute_reply.started":"2024-01-09T23:42:30.883077Z","shell.execute_reply":"2024-01-09T23:42:30.893626Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print('Building Model Object!')\n\nmodel_params = {'gbr': {'n_estimators': 409,\n  'learning_rate': 0.07361180235738161,\n  'max_depth': 10,\n  'min_samples_split': 0.6482497101518737,\n  'min_samples_leaf': 0.31972085330435496,\n  'subsample': 0.9055662628017166},\n 'rfr': {'n_estimators': 425,\n  'max_depth': 9,\n  'min_samples_split': 0.4596197102513105,\n  'min_samples_leaf': 0.22695246564074448},\n 'lgbm': {'boosting_type': 'dart',\n  'n_estimators': 115,\n  'learning_rate': 0.0221079754217087,\n  'num_leaves': 96,\n  'max_depth': 12,\n  'min_child_samples': 7,\n  'subsample': 0.9063815116324397,\n  'colsample_bytree': 0.5191190249507341,\n  'reg_alpha': 0.7733601728487565,\n  'reg_lambda': 0.745926710711842}}\n\nreg_model = RegressorEnsemble(model_params=model_params).fit(X_train, y)\n\ny_hat_train = np.round(reg_model.predict(X_train), 3)\ny_hat_test = np.round(reg_model.predict(X_test), 3)\n\nsubmission_data = pd.DataFrame({'id': master_data_test.index, 'score': y_hat_test})\n\nprint('Previous Best:: R^2 Score: 0.861, RMSE Score: 0.382, Rounded RMSE Score: 0.39')\n\nprint(f'R^2 Score: {round(reg_model.score(X_train, y), 3)},', \n      f'RMSE Score: {round(mean_squared_error(y, y_hat_train, squared=False), 3)},',\n      f'Rounded RMSE Score: {round(rounded_rmse(y, y_hat_train), 3)}')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:42:30.895649Z","iopub.execute_input":"2024-01-09T23:42:30.896133Z","iopub.status.idle":"2024-01-09T23:42:37.189527Z","shell.execute_reply.started":"2024-01-09T23:42:30.896105Z","shell.execute_reply":"2024-01-09T23:42:37.188409Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Building Model Object!\nPrevious Best:: R^2 Score: 0.861, RMSE Score: 0.382, Rounded RMSE Score: 0.39\nR^2 Score: 0.667, RMSE Score: 0.592, Rounded RMSE Score: 0.616\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:42:37.192765Z","iopub.execute_input":"2024-01-09T23:42:37.193085Z","iopub.status.idle":"2024-01-09T23:42:37.203216Z","shell.execute_reply.started":"2024-01-09T23:42:37.193057Z","shell.execute_reply":"2024-01-09T23:42:37.202066Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"         id  score\n0  0000aaaa  1.565\n1  2222bbbb  1.528\n2  4444cccc  1.528","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.565</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.528</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.528</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_data.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:42:37.204893Z","iopub.execute_input":"2024-01-09T23:42:37.205341Z","iopub.status.idle":"2024-01-09T23:42:37.213555Z","shell.execute_reply.started":"2024-01-09T23:42:37.205301Z","shell.execute_reply":"2024-01-09T23:42:37.212480Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}